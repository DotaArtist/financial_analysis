import os
import torch
import torch.distributed.checkpoint as dist_cp
from transformers import AutoModelForCausalLM, AutoTokenizer

# ================= é…ç½®åŒºåŸŸ =================
BASE_MODEL_PATH = "/workspace/training/pretrained_model/gemma-3-12b-it-sp-end"
CHECKPOINT_DIR = "/workspace/training/models/dpo_gemma3_12b_it_sp_end/checkpoint-3318"
SAVE_PATH = "/workspace/training/models/gemma-3-12b-it-sp-end-dpov1130"
# ===========================================

def inspect_checkpoint_keys(fsdp_path):
    """è¯»å– Checkpoint å…ƒæ•°æ®ï¼Œè¿”å›æ‰€æœ‰å­˜åœ¨çš„ key"""
    print(f"ğŸ” æ­£åœ¨è¯»å– Checkpoint å…ƒæ•°æ®: {fsdp_path}")
    try:
        reader = dist_cp.FileSystemReader(fsdp_path)
        metadata = reader.read_metadata()
        return set(metadata.state_dict_metadata.keys())
    except Exception as e:
        print(f"âŒ è¯»å–å…ƒæ•°æ®å¤±è´¥: {e}")
        return set()

def main():
    print(f"1. åˆå§‹åŒ–åŸºç¡€æ¨¡å‹: {BASE_MODEL_PATH} ...")
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        device_map="cpu",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
    )

    # è·å–æ¨¡å‹æœŸæœ›çš„ Key
    model_keys = set(model.state_dict().keys())

    fsdp_weights_path = os.path.join(CHECKPOINT_DIR, "pytorch_model_fsdp_0")
    if not os.path.exists(fsdp_weights_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æƒé‡è·¯å¾„: {fsdp_weights_path}")

    # 2. è·å– Checkpoint ä¸­å®é™…å­˜åœ¨çš„ Key
    checkpoint_keys = inspect_checkpoint_keys(fsdp_weights_path)

    if not checkpoint_keys:
        print("âŒ æ— æ³•è·å– Checkpoint keysï¼Œè„šæœ¬ç»ˆæ­¢ã€‚")
        return

    print(f"ğŸ“Š ç»Ÿè®¡: æ¨¡å‹æœŸæœ› {len(model_keys)} ä¸ªå‚æ•°, Checkpoint åŒ…å« {len(checkpoint_keys)} ä¸ªå‚æ•°")

    # 3. æ„å»ºåŠ è½½å­—å…¸ (Mapping)
    state_dict_to_load = {}

    # =======================================================
    # æ ¸å¿ƒä¿®å¤ï¼šæ›´æ–°åŒ¹é…é€»è¾‘ï¼Œå¤„ç† 'model.' å‰ç¼€
    # =======================================================
    def find_matching_key(target_key, ckpt_keys):
        # 1. å°è¯•ç›´æ¥åŒ¹é…
        if target_key in ckpt_keys:
            return target_key

        # 2. ã€ä¿®å¤ç‚¹ã€‘å°è¯•æ·»åŠ  'model.' å‰ç¼€
        # ä½ çš„æ—¥å¿—æ˜¾ç¤º checkpoint é‡Œçš„ key å¤šäº† 'model.' å¼€å¤´
        prefix_key = "model." + target_key
        if prefix_key in ckpt_keys:
            return prefix_key

        # 3. å°è¯•å…¶ä»–å¸¸è§çš„ FSDP å‰ç¼€
        if ("_fsdp_wrapped_module." + target_key) in ckpt_keys:
            return "_fsdp_wrapped_module." + target_key

        return None
    # =======================================================

    print("ğŸ› ï¸  æ­£åœ¨æ„å»ºå‚æ•°æ˜ å°„...")

    mapped_count = 0
    missing_keys = []

    original_state_dict = model.state_dict()

    for model_key, tensor in original_state_dict.items():
        # è·³è¿‡ lm_head (Weight Tying é—®é¢˜)ï¼Œé˜²æ­¢ FSDP åŠ è½½æŠ¥é”™
        if "lm_head.weight" in model_key:
            continue

        found_key = find_matching_key(model_key, checkpoint_keys)

        if found_key:
            # å»ºç«‹æ˜ å°„: Checkpoint Key -> Model Tensor
            state_dict_to_load[found_key] = tensor
            mapped_count += 1
        else:
            missing_keys.append(model_key)

    print(f"âœ… æˆåŠŸæ˜ å°„ {mapped_count} ä¸ªå‚æ•°ã€‚")

    if missing_keys:
        print(f"âš ï¸  ä»¥ä¸‹ {len(missing_keys)} ä¸ªå‚æ•°åœ¨ Checkpoint ä¸­æœªæ‰¾åˆ° (å‰5ä¸ª):")
        for k in missing_keys[:5]:
            print(f"   - {k}")

    if mapped_count == 0:
        print("âŒ ä¾ç„¶æ²¡æœ‰åŒ¹é…æˆåŠŸï¼Œè¯·æ£€æŸ¥è„šæœ¬é€»è¾‘ã€‚")
        return

    # 4. æ‰§è¡ŒåŠ è½½
    print("ğŸš€ å¼€å§‹åŠ è½½æƒé‡ (dist_cp.load)...")
    # æ³¨æ„ï¼šdist_cp.load ä¼šç›´æ¥æŠŠæ•°æ®å†™å…¥ state_dict_to_load çš„ values (å³ model çš„ tensor)
    dist_cp.load(
        state_dict=state_dict_to_load,
        checkpoint_id=fsdp_weights_path,
    )

    # 5. æ‰‹åŠ¨ä¿®å¤ Weight Tying (lm_head)
    print("ğŸ”— é‡æ–°ç»‘å®š lm_head æƒé‡...")
    try:
        # Gemma 3 ç»“æ„é€šå¸¸æ˜¯ model.language_model.model.embed_tokens
        # ä½†æˆ‘ä»¬è¿™é‡Œæ“ä½œçš„æ˜¯ AutoModel åŠ è½½çš„å¯¹è±¡
        if hasattr(model, "language_model"):
            model.language_model.lm_head.weight = model.language_model.model.embed_tokens.weight
        elif hasattr(model, "lm_head"):
            model.lm_head.weight = model.model.embed_tokens.weight
        print("   -> ç»‘å®šæˆåŠŸ")
    except Exception as e:
        print(f"   -> [è­¦å‘Š] è‡ªåŠ¨ç»‘å®šå¤±è´¥ï¼Œè¯·ç¡®è®¤æ¨¡å‹ç»“æ„: {e}")

    # 6. ä¿å­˜
    print(f"ğŸ’¾ ä¿å­˜ Safetensors è‡³: {SAVE_PATH}")
    model.save_pretrained(SAVE_PATH, safe_serialization=True, max_shard_size="5GB")

    try:
        tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_DIR, trust_remote_code=True)
    except:
        print("Checkpoint ä¸­æ—  tokenizerï¼Œä» Base Model å¤åˆ¶...")
        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)

    tokenizer.save_pretrained(SAVE_PATH)

    print("âœ¨ å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    main()
